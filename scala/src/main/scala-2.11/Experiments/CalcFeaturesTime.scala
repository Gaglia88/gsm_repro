package Experiments

import java.io.PrintWriter
import java.util.Calendar

import SparkER.BlockBuildingMethods.TokenBlocking
import SparkER.BlockRefinementMethods.PruningMethods._
import SparkER.BlockRefinementMethods.{BlockFiltering, BlockPurging}
import SparkER.DataStructures.{BlockAbstract, Profile, ProfileBlocks}
import SparkER.Utilities.Converters
import SparkER.Wrappers.{CSVWrapper, JSONWrapper}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ListBuffer
import scala.io.Source
import scala.util.parsing.json.JSON


/**
  * Experiments
  *
  * @author Luca Gagliardelli
  * @since 18/12/2018
  **/
object CalcFeaturesTime {

  case class Dataset(name: String, basePath: String, dataset1: String, dataset2: String, groundtruth: String, dtype: String, purgingThreshold: Double = 1.0, idField: String = "realProfileID")

  def base(dataset: Dataset): (RDD[Profile], RDD[Profile], RDD[Profile], Array[Long], Int, Set[(Long, Long)], Broadcast[Set[(Long, Long)]], RDD[ProfileBlocks], RDD[BlockAbstract]) = {

    val sc = SparkContext.getOrCreate()

    //Carico i dati
    val dataset1 = {
      if (dataset.dtype == "json") {
        JSONWrapper.loadProfiles(dataset.basePath + dataset.dataset1, realIDField = dataset.idField, sourceId = 1)
      }
      else {
        CSVWrapper.loadProfiles2(dataset.basePath + dataset.dataset1, realIDField = dataset.idField, sourceId = 1, header = true)
      }
    }

    //Separatore
    val maxIdDataset1 = dataset1.map(_.id).max()

    val dataset2 = {
      if (dataset.dtype == "json") {
        JSONWrapper.loadProfiles(dataset.basePath + dataset.dataset2, realIDField = dataset.idField, sourceId = 2, startIDFrom = maxIdDataset1 + 1)
      }
      else {
        CSVWrapper.loadProfiles2(dataset.basePath + dataset.dataset2, realIDField = dataset.idField, sourceId = 2, startIDFrom = maxIdDataset1 + 1, header = true)
      }
    }

    //ID massimo dei profili
    val maxID = dataset2.map(_.id).max().toInt

    //Definisco i separatori
    val separators = Array(maxIdDataset1)

    val profiles = dataset1.union(dataset2)


    //Carico il groundtruth
    val groundtruth = {
      if (dataset.dtype == "json") {
        JSONWrapper.loadGroundtruth(dataset.basePath + dataset.groundtruth, firstDatasetAttribute = "id1", secondDatasetAttribute = "id2")
      }
      else {
        CSVWrapper.loadGroundtruth(dataset.basePath + dataset.groundtruth)
      }
    }

    //Converts the ids in the groundtruth to the autogenerated ones
    val realIdIds1 = sc.broadcast(dataset1.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    val realIdIds2 = sc.broadcast(dataset2.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    var newGT: Set[(Long, Long)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds1.value.get(g.firstEntityID)
      val second = realIdIds2.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) {
          (f, s)
        }
        else {
          (s, f)
        }
      }
      else {
        (-1L, -1L)
      }
    }.filter(_._1 >= 0).collect().toSet


    //Invio il groundtruth in broadcast per dopo
    val gt = sc.broadcast(newGT)

    realIdIds1.unpersist(true)
    realIdIds2.unpersist(true)


    //Token blocking
    val blocks = TokenBlocking.createBlocks(profiles, separators)


    val blocksPurged = BlockPurging.blockPurging(blocks, dataset.purgingThreshold)

    val profileBlocks = Converters.blocksToProfileBlocks(blocksPurged)
    val profileBlocksFiltered = BlockFiltering.blockFiltering(profileBlocks, 0.8)
    val blocksAfterFiltering = Converters.profilesBlockToBlocks(profileBlocksFiltered, separators)

    (dataset1, dataset2, profiles, separators, maxID, newGT, gt, profileBlocksFiltered, blocksAfterFiltering)
  }


  def generateFeatures(dataset: Dataset, feats: List[(Int, String)], out: PrintWriter): Unit = {

    val sc = SparkContext.getOrCreate()
    val sparkSession = SparkSession.builder().getOrCreate()

    val tstart = Calendar.getInstance()
    val (dataset1, dataset2, profiles, separators, maxID, newGT, gt, profileBlocksFiltered, blocksAfterFiltering) = base(dataset)


    //Questo serve per riconvertire le coppie con gli id originali
    val profilesIds = sc.broadcast(profiles.map(p => (p.id, p.originalID)).collectAsMap())

    //Numero di blocchi
    val blocksNum = blocksAfterFiltering.count().toDouble

    //Invio in broadcast i blocchi
    val blockIndexMap = blocksAfterFiltering.map(b => (b.blockID, b.profiles)).collectAsMap()
    val blockIndex = sc.broadcast(blockIndexMap)

    //Mi serve anche come dato in broadcast il numero di blocchi per ogni profilo
    val profileBlocksNumIndex = sc.broadcast(profileBlocksFiltered.map(pb => (pb.profileID, pb.blocks.size)).collectAsMap())


    val tbase = Calendar.getInstance()
    val blockingTime = (tbase.getTimeInMillis - tstart.getTimeInMillis) / 1000.0


    feats.foreach { f =>
      val featID = f._1
      val inputFeatures = f._2.split(",").map(_.trim)
      val t1 = Calendar.getInstance()

      val totalComparisons: Double = {
        if (inputFeatures.contains("aejs")) {
          blocksAfterFiltering.map(b => b.getComparisonSize()).sum()
        }
        else {
          0.0
        }
      }

      var invProfileBlockSizeIndex: Broadcast[scala.collection.Map[Long, Double]] = null
      if (inputFeatures.contains("nrs")) {
        invProfileBlockSizeIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
          b.getAllProfiles.map { p =>
            (p, 1.0 / b.size)
          }
        }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())
      }

      var comparisonsPerProfileIndex: Broadcast[scala.collection.Map[Long, Double]] = null

      if (inputFeatures.contains("aejs")) {
        comparisonsPerProfileIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
          b.getAllProfiles.map { p =>
            (p, b.getComparisonSize(p))
          }
        }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())
      }

      var inverseProfileBlockCompSizeIndex: Broadcast[scala.collection.Map[Long, Double]] = null
      if (inputFeatures.contains("wjs")) {
        inverseProfileBlockCompSizeIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
          b.getAllProfiles.map { p =>
            (p, 1.0 / b.getComparisonSize())
          }
        }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())
      }


      var profilesStatsBrd: Broadcast[scala.collection.Map[Int, (Double, Double)]] = null

      if (inputFeatures.contains("js") || inputFeatures.contains("numCompP1")) {
        //Calcolo le statistiche per ogni profilo. Il numero di comparison ridondanti e non ridondanti
        val profilesStats = profileBlocksFiltered.mapPartitions { part =>
          val seen = {
            if (inputFeatures.contains("numCompP1")) {
              Array.fill[Boolean](maxID + 1) {
                false
              }
            }
            else {
              Array.empty[Boolean]
            }
          }
          val neighbors = {
            if (inputFeatures.contains("numCompP1")) {
              Array.ofDim[Int](maxID + 1)
            }
            else {
              Array.empty[Int]
            }
          }

          var neighborsNumber = 0

          //For each profile
          part.map { pb =>
            val profileID = pb.profileID.toInt
            var redudantComparison: Double = 0
            var nonRedudantComparison: Double = 0

            //For each block
            pb.blocks.foreach { block =>
              //Block ID
              val blockID = block.blockID
              //Gets the block data from the index
              val blockData = blockIndex.value.get(blockID)
              if (blockData.isDefined) {
                //Gets all the neighbors, i.e. other profiles in the block
                PruningUtils.getAllNeighbors(profileID, blockData.get, separators).foreach { neighbourID =>
                  if (inputFeatures.contains("numCompP1")) {
                    //If the neighbor is new it is a non-redudant comparison
                    if (!seen(neighbourID.toInt)) {
                      nonRedudantComparison += 1
                      seen.update(neighbourID.toInt, true)
                      neighbors.update(neighborsNumber, neighbourID.toInt)
                      neighborsNumber += 1
                    }
                  }
                  redudantComparison += 1
                }
              }
            }

            if (inputFeatures.contains("numCompP1")) {
              //Resets all
              for (i <- 0 until neighborsNumber) {
                seen.update(neighbors(i), false)
              }
              neighborsNumber = 0
            }

            (profileID, redudantComparison, nonRedudantComparison)
          }
        }

        //Invio in broadcast le info, serviranno dopo
        profilesStatsBrd = sc.broadcast(profilesStats.map(x => (x._1, (x._2, x._3))).collectAsMap())
      }

      //For each partition
      val pairsFeatures = profileBlocksFiltered.mapPartitions { part =>

        val features: ListBuffer[(String, String, Double, Double, Double, Double, Double, Double, Double, Double, Double, Int)] = ListBuffer[(String, String, Double, Double, Double, Double, Double, Double, Double, Double, Double, Int)]()

        val CBS = Array.fill[Double](maxID + 1) {
          0
        }
        val RACCB: Array[Double] = {
          if (inputFeatures.contains("raccb") || inputFeatures.contains("wjs")) {
            Array.fill[Double](maxID + 1) {
              0
            }
          }
          else {
            Array.empty[Double]
          }
        }
        val RS: Array[Double] = {
          if (inputFeatures.contains("rs") || inputFeatures.contains("nrs")) {
            Array.fill[Double](maxID + 1) {
              0
            }
          }
          else {
            Array.empty[Double]
          }
        }

        val neighbors = Array.ofDim[Int](maxID + 1)
        var neighborsNumber = 0

        //For each profile
        part.foreach { pb =>
          val profileID = pb.profileID.toInt

          //For each block
          pb.blocks.foreach { block =>
            //Block ID
            val blockID = block.blockID
            //Gets the block data from the index
            val blockData = blockIndex.value.get(blockID)
            if (blockData.isDefined) {
              //Gets all the neighbors, i.e. other profiles in the block
              PruningUtils.getAllNeighbors(profileID, blockData.get, separators).foreach { neighbourID =>
                if (profileID < neighbourID) {
                  CBS.update(neighbourID.toInt, CBS(neighbourID.toInt) + 1)
                  if (CBS(neighbourID.toInt) == 1) {
                    if (inputFeatures.contains("raccb") || inputFeatures.contains("wjs")) {
                      RACCB.update(neighbourID.toInt, RACCB(neighbourID.toInt) + 1.0 / block.comparisons)
                    }
                    if (inputFeatures.contains("RS") || inputFeatures.contains("nrs")) {
                      RS.update(neighbourID.toInt, RS(neighbourID.toInt) + 1.0 / blockIndex.value(blockID).flatten.length)
                    }
                    neighbors.update(neighborsNumber, neighbourID.toInt)
                    neighborsNumber += 1
                  }
                }
              }
            }
          }

          //log.info"SPARKER - PART STATS DONE")
          val ibf1 = math.log(blocksNum / profileBlocksNumIndex.value(profileID))


          var compP1 = (0.0, 0.0)
          if (inputFeatures.contains("JS") || inputFeatures.contains("numCompP1")) {
            //log.info"SPARKER - READ COMP1")
            compP1 = profilesStatsBrd.value(profileID)
          }

          var compP2 = (0.0, 0.0)
          for (i <- 0 until neighborsNumber) {


            if (inputFeatures.contains("JS") || inputFeatures.contains("numCompP1")) {
              //log.info"SPARKER - READ COMP2")
              compP2 = profilesStatsBrd.value(neighbors(i))
            }

            val ibf2 = math.log(blocksNum / profileBlocksNumIndex.value(neighbors(i)))
            val CFIBF = CBS(neighbors(i)) * ibf1 * ibf2

            var raccb: Double = 0
            if (inputFeatures.contains("raccb") || inputFeatures.contains("WJS")) {
              //log.info"SPARKER - CALC RACCB")
              raccb = RACCB(neighbors(i))
              if (raccb < 1.0E-6) {
                raccb = 1.0E-6
              }
            }


            var JS = 0.0
            if (inputFeatures.contains("js")) {
              //log.info"SPARKER - CALC JS")
              JS = CBS(neighbors(i)) / (compP1._1 + compP2._1 - CBS(neighbors(i)))
            }

            //Nuove misure
            var rs = 0.0
            if (inputFeatures.contains("rs") || inputFeatures.contains("nrs")) {
              //log.info"SPARKER - CALC RS")
              rs = RS(neighbors(i))
            }

            var NRS = 0.0
            if (inputFeatures.contains("nrs")) {
              //log.info"SPARKER - CALC NRS")
              NRS = rs / (invProfileBlockSizeIndex.value(profileID) + invProfileBlockSizeIndex.value(neighbors(i)) - rs)
            }

            var WJS = 0.0
            if (inputFeatures.contains("wjs")) {
              //log.info"SPARKER - CALC WJS")
              WJS = raccb / (inverseProfileBlockCompSizeIndex.value(profileID) + inverseProfileBlockCompSizeIndex.value(neighbors(i)) - raccb)
            }

            var AEJS = 0.0
            if (inputFeatures.contains("aejs")) {
              //log.info"SPARKER - CALC AEJS")
              val JS_1 = CBS(neighbors(i)) / (profileBlocksNumIndex.value(profileID) + profileBlocksNumIndex.value(profileID) - CBS(neighbors(i)))
              AEJS = JS_1 * math.log(totalComparisons / comparisonsPerProfileIndex.value(profileID)) * math.log(totalComparisons / comparisonsPerProfileIndex.value(neighbors(i)))
            }


            //log.info"SPARKER - CALC ISMATCH")
            val isMatch = if (gt.value.contains((profileID, neighbors(i)))) 1 else 0

            //Emetto la feature per questa coppia: ID PROFILO 1, ID PROFILO 2, CFIBF, RACCB, JS, Numero di comparison non ridondanti di P1, Numero di comparison non ridondanti di P2, Match o meno
            //log.info"SPARKER - APPEND FEATURES")
            features.append((profilesIds.value(profileID), profilesIds.value(neighbors(i)), CFIBF, raccb, JS, compP1._2, compP2._2, rs, AEJS, NRS, WJS, isMatch))

            if (inputFeatures.contains("raccb") || inputFeatures.contains("wjs")) {
              //log.info"SPARKER - CLEAN RACCB")
              RACCB.update(neighbors(i), 0)
            }

            if (inputFeatures.contains("rs") || inputFeatures.contains("nrs")) {
              //log.info"SPARKER - CLEAN RS")
              RS.update(neighbors(i), 0)
            }

            CBS.update(neighbors(i), 0)
          }
          neighborsNumber = 0
        }

        features.toIterator
      }

      val featuresDF = sparkSession.createDataFrame(pairsFeatures).toDF("p1", "p2", "cfibf", "raccb", "js", "numCompP1", "numCompP2", "rs", "aejs", "nrs", "wjs", "is_match")
      featuresDF.count()

      val t2 = Calendar.getInstance()

      val featuresTime = (t2.getTimeInMillis - t1.getTimeInMillis) / 1000.0
	  
	  featuresDF.unpersist(true)


      if (profilesStatsBrd != null) {
        profilesStatsBrd.unpersist(true)
      }


      if (invProfileBlockSizeIndex != null) {
        invProfileBlockSizeIndex.unpersist(true)
      }

      if (inverseProfileBlockCompSizeIndex != null) {
        inverseProfileBlockCompSizeIndex.unpersist(true)
      }

      if (comparisonsPerProfileIndex != null) {
        comparisonsPerProfileIndex.unpersist(true)
      }

      out.println(dataset.name + ";" + featID + ";" + inputFeatures.mkString(",") + ";" + blockingTime + ";" + featuresTime)
      out.flush()
    }

    profilesIds.unpersist(true)
    blockIndex.unpersist(true)
    profileBlocksNumIndex.unpersist(true)
    gt.unpersist(true)
  }


  def main(args: Array[String]): Unit = {
    val config = Source.fromFile("/home/app/config/config.ini")
    val spark_max_memory = config.getLines().filter(_.startsWith("max_memory=")).next().replace("max_memory=", "")
    config.close()

    val input = Source.fromFile("/home/app/datasets/datasets.json")
    val lines = input.mkString
    input.close()
    val json = JSON.parseFull(lines)

    val datasets = json.get.asInstanceOf[List[Map[String, String]]].map(d => {
      val name = d("name")
      val basePath = "/home/app/" + d("base_path") + "/"
      val dataset1 = d("d1")
      val dataset2 = d.getOrElse("d2", "")
      val groundtruth = d("gt")
      val dtype = d("format")
      val idField = d("d1_id_field")

      Dataset(name, basePath, dataset1, dataset2, groundtruth, dtype, idField = idField)
    })

    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.driver.memory", spark_max_memory)
      .set("spark.executor.memory", spark_max_memory)
      .set("spark.local.dir", "/home/app/tmp")
      .set("spark.driver.maxResultSize", "0")
	  .set("spark.executor.instances", "1")


    val sc = new SparkContext(conf)

    val features = sc.textFile("/home/app/config/feature_sets.csv")
    val feats = features.map(x => x.split(";")).filter(x => !x.head.contains("conf_id")).map(x => (x.head.toInt, x.last)).collect().toList


    val out = new PrintWriter("/home/app/results/features_calc_time.csv")
    out.println("dataset;conf_id;features;blockingTime;featuresTime")

    datasets.filter(d => List("Movies", "WalmartAmazon").contains(d.name)).foreach { d =>
      generateFeatures(d, feats, out)
    }

    out.close()
    sc.stop()
  }
}
