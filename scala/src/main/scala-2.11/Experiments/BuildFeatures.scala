package Experiments

import java.util.Calendar

import SparkER.BlockBuildingMethods.TokenBlocking
import SparkER.BlockRefinementMethods.PruningMethods._
import SparkER.BlockRefinementMethods.{BlockFiltering, BlockPurging}
import SparkER.Utilities.Converters
import SparkER.Wrappers.{CSVWrapper, JSONWrapper}
import org.apache.spark.sql.SparkSession
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.mutable.ListBuffer
import scala.io.Source
import scala.util.parsing.json.JSON
import java.io.PrintWriter

import SparkER.DataStructures.Profile
import breeze.numerics.log
import org.apache.spark.rdd.RDD

import java.nio.file.Paths
import java.nio.file.Files


/**
  * Experiments
  *
  * @author Luca Gagliardelli
  * @since 18/12/2018
  **/
object BuildFeatures {

  case class Dataset(name: String, basePath: String, dataset1: String, dataset2: String, groundtruth: String, format: String, dtype: String, gt_d1_field: String, gt_d2_field: String, purgingThreshold: Double = 1.0, idField: String = "realProfileID")

  def generateFeatures(dataset: Dataset, outputPath: String): (String, Long, Long, Int, Long, Double, Double, Double, Double, Double, Double) = {

    val sc = SparkContext.getOrCreate()
    val sparkSession = SparkSession.builder().getOrCreate()

    //Carico i dati
    val dataset1 = {
      if (dataset.format == "json") {
        JSONWrapper.loadProfiles(dataset.basePath + dataset.dataset1, realIDField = dataset.idField, sourceId = 1)
      }
      else {
        CSVWrapper.loadProfiles2(dataset.basePath + dataset.dataset1, realIDField = dataset.idField, sourceId = 1, header = true)
      }
    }

    //Separatore
    val maxIdDataset1 = dataset1.map(_.id).max()

    val dataset2: RDD[Profile] = {
      if (dataset.dtype == "clean") {
        if (dataset.format == "json") {
          JSONWrapper.loadProfiles(dataset.basePath + dataset.dataset2, realIDField = dataset.idField, sourceId = 2, startIDFrom = maxIdDataset1 + 1)
        }
        else {
          CSVWrapper.loadProfiles2(dataset.basePath + dataset.dataset2, realIDField = dataset.idField, sourceId = 2, startIDFrom = maxIdDataset1 + 1, header = true)
        }
      }
      else {
        null
      }
    }

    //ID massimo dei profili
    val maxID = {
      if (dataset.dtype == "clean") {
        dataset2.map(_.id).max().toInt
      }
      else {
        maxIdDataset1.toInt
      }
    }

    //Definisco i separatori
    val separators = {
      if (dataset.dtype == "clean") {
        Array(maxIdDataset1)
      }
      else {
        Array.emptyLongArray
      }
    }

    val profiles = {
      if (dataset.dtype == "clean") {
        dataset1.union(dataset2)
      }
      else {
        dataset1
      }
    }


    //Carico il groundtruth
    val groundtruth = {
      if (dataset.format == "json") {
        JSONWrapper.loadGroundtruth(dataset.basePath + dataset.groundtruth, firstDatasetAttribute = dataset.gt_d1_field, secondDatasetAttribute = dataset.gt_d2_field)
      }
      else {
        CSVWrapper.loadGroundtruth(dataset.basePath + dataset.groundtruth)
      }
    }

    //Converts the ids in the groundtruth to the autogenerated ones
    val realIdIds1 = sc.broadcast(dataset1.map { p =>
      (p.originalID, p.id)
    }.collectAsMap())

    val realIdIds2 = {
      if (dataset.dtype == "clean") {
        sc.broadcast(dataset2.map { p =>
          (p.originalID, p.id)
        }.collectAsMap())
      }
      else {
        realIdIds1
      }
    }

    var newGT: Set[(Long, Long)] = null
    newGT = groundtruth.map { g =>
      val first = realIdIds1.value.get(g.firstEntityID)
      val second = realIdIds2.value.get(g.secondEntityID)
      if (first.isDefined && second.isDefined) {
        val f = first.get
        val s = second.get
        if (f < s) {
          (f, s)
        }
        else {
          (s, f)
        }
      }
      else {
        (-1L, -1L)
      }
    }.filter(_._1 >= 0).collect().toSet


    //Invio il groundtruth in broadcast per dopo
    val gt = sc.broadcast(newGT)

    realIdIds1.unpersist()
    realIdIds2.unpersist()

    val t1 = Calendar.getInstance()

    //Token blocking
    val blocks = TokenBlocking.createBlocks(profiles, separators)


    val blocksPurged = BlockPurging.blockPurging(blocks, dataset.purgingThreshold)

    val profileBlocks = Converters.blocksToProfileBlocks(blocksPurged)
    val profileBlocksFiltered = BlockFiltering.blockFiltering(profileBlocks, 0.8)
    val blocksAfterFiltering = Converters.profilesBlockToBlocks(profileBlocksFiltered, separators)


    val comparisonsPerProfileIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
      b.getAllProfiles.map { p =>
        (p, b.getComparisonSize(p))
      }
    }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())

    val invProfileBlockSizeIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
      b.getAllProfiles.map { p =>
        (p, 1.0 / b.size)
      }
    }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())

    val totalComparisons = blocksAfterFiltering.map(b => b.getComparisonSize()).sum()

    //Number of blocks
    val blocksNum = blocksAfterFiltering.count().toDouble


    val inverseProfileBlockCompSizeIndex = sc.broadcast(blocksAfterFiltering.flatMap { b =>
      b.getAllProfiles.map { p =>
        (p, 1.0 / b.getComparisonSize())
      }
    }.groupByKey().map(x => (x._1, x._2.sum)).collectAsMap())

    //Invio in broadcast i blocchi
    val blockIndexMap = blocksAfterFiltering.map(b => (b.blockID, b.profiles)).collectAsMap()
    val blockIndex = sc.broadcast(blockIndexMap)

    //Mi serve anche come dato in broadcast il numero di blocchi per ogni profilo
    val profileBlocksNumIndex = sc.broadcast(profileBlocksFiltered.map(pb => (pb.profileID, pb.blocks.size)).collectAsMap())

    //Calcolo le statistiche per ogni profilo. Il numero di comparison ridondanti e non ridondanti
    val profilesStats = profileBlocksFiltered.mapPartitions { part =>
      val seen = Array.fill[Boolean](maxID + 1) {
        false
      }
      val neighbors = Array.ofDim[Int](maxID + 1)
      var neighborsNumber = 0

      //For each profile
      part.map { pb =>
        val profileID = pb.profileID.toInt
        var redudantComparison: Double = 0
        var nonRedudantComparison: Double = 0

        //For each block
        pb.blocks.foreach { block =>
          //Block ID
          val blockID = block.blockID
          //Gets the block data from the index
          val blockData = blockIndex.value.get(blockID)
          if (blockData.isDefined) {
            //Gets all the neighbors, i.e. other profiles in the block
            PruningUtils.getAllNeighbors(profileID, blockData.get, separators).foreach { neighbourID =>
              //If the neighbor is new it is a non-redudant comparison
              if (!seen(neighbourID.toInt)) {
                nonRedudantComparison += 1
                seen.update(neighbourID.toInt, true)
                neighbors.update(neighborsNumber, neighbourID.toInt)
                neighborsNumber += 1
              }
              redudantComparison += 1
            }
          }
        }

        //Resets all
        for (i <- 0 until neighborsNumber) {
          seen.update(neighbors(i), false)
        }
        neighborsNumber = 0

        (profileID, redudantComparison, nonRedudantComparison)
      }
    }

    //Invio in broadcast le info, serviranno dopo
    val profilesStatsBrd = sc.broadcast(profilesStats.map(x => (x._1, (x._2, x._3))).collectAsMap())

    //Questo serve per riconvertire le coppie con gli id originali
    val profilesIds = sc.broadcast(profiles.map(p => (p.id, p.originalID)).collectAsMap())

    //For each partition
    val pairsFeatures = profileBlocksFiltered.mapPartitions { part =>
      val features: ListBuffer[(String, String, Double, Double, Double, Double, Double, Double, Double, Double, Double, Int)] = ListBuffer[(String, String, Double, Double, Double, Double, Double, Double, Double, Double, Double, Int)]()

      val CBS = Array.fill[Double](maxID + 1) {
        0
      }
      val RACCB = Array.fill[Double](maxID + 1) {
        0
      }
      val RS = Array.fill[Double](maxID + 1) {
        0
      }
      val neighbors = Array.ofDim[Int](maxID + 1)
      var neighborsNumber = 0

      //For each profile
      part.foreach { pb =>
        val profileID = pb.profileID.toInt

        //For each block
        pb.blocks.foreach { block =>
          //Block ID
          val blockID = block.blockID
          //Gets the block data from the index
          val blockData = blockIndex.value.get(blockID)
          if (blockData.isDefined) {
            //Gets all the neighbors, i.e. other profiles in the block
            PruningUtils.getAllNeighbors(profileID, blockData.get, separators).foreach { neighbourID =>
              if (profileID < neighbourID) {
                CBS.update(neighbourID.toInt, CBS(neighbourID.toInt) + 1)
                if (CBS(neighbourID.toInt) == 1) {
                  RACCB.update(neighbourID.toInt, RACCB(neighbourID.toInt) + 1.0 / block.comparisons)
                  RS.update(neighbourID.toInt, RS(neighbourID.toInt) + 1.0 / blockIndex.value(blockID).flatten.length)
                  neighbors.update(neighborsNumber, neighbourID.toInt)
                  neighborsNumber += 1
                }
              }
            }
          }
        }

        val ibf1 = math.log(blocksNum / profileBlocksNumIndex.value(profileID))
        val dataP1 = profilesStatsBrd.value(profileID)

        for (i <- 0 until neighborsNumber) {
          val ibf2 = math.log(blocksNum / profileBlocksNumIndex.value(neighbors(i)))
          val CFIBF = CBS(neighbors(i)) * ibf1 * ibf2

          var raccb = RACCB(neighbors(i))
          if (raccb < 1.0E-6) {
            raccb = 1.0E-6
          }


          val dataP2 = profilesStatsBrd.value(neighbors(i))
          val JS = CBS(neighbors(i)) / (dataP1._1 + dataP2._1 - CBS(neighbors(i)))

          //Nuove misure
          val JS_1 = CBS(neighbors(i)) / (profileBlocksNumIndex.value(profileID) + profileBlocksNumIndex.value(profileID) - CBS(neighbors(i)))
          val AEJS = JS_1 * math.log(totalComparisons / comparisonsPerProfileIndex.value(profileID)) * math.log(totalComparisons / comparisonsPerProfileIndex.value(neighbors(i)))
          val rs = RS(neighbors(i))
          val NRS = rs / (invProfileBlockSizeIndex.value(profileID) + invProfileBlockSizeIndex.value(neighbors(i)) - rs)
          val WJS = raccb / (inverseProfileBlockCompSizeIndex.value(profileID) + inverseProfileBlockCompSizeIndex.value(neighbors(i)) - raccb)


          val isMatch = if (gt.value.contains((profileID, neighbors(i)))) 1 else 0

          //Emetto la feature per questa coppia: ID PROFILO 1, ID PROFILO 2, CFIBF, RACCB, JS, Numero di comparison non ridondanti di P1, Numero di comparison non ridondanti di P2, Match o meno
          features.append((profilesIds.value(profileID), profilesIds.value(neighbors(i)), CFIBF, raccb, JS, dataP1._2, dataP2._2, rs, AEJS, NRS, WJS, isMatch))

          RACCB.update(neighbors(i), 0)
          RS.update(neighbors(i), 0)
          CBS.update(neighbors(i), 0)
        }
        neighborsNumber = 0
      }

      features.toIterator
    }

    val featuresDF = sparkSession.createDataFrame(pairsFeatures).toDF("p1", "p2", "cfibf", "raccb", "js", "numCompP1", "numCompP2", "rs", "aejs", "nrs", "wjs", "is_match")

    featuresDF.coalesce(1).write.parquet(outputPath + dataset.name)

    val t2 = Calendar.getInstance()

    val (pc, pq, numEdges) = PCPQBlockCalc.getPcPq(blocksAfterFiltering, newGT, maxID, separators)

    val numBlocks = blocksAfterFiltering.count()
    val numComp = blocksAfterFiltering.map(_.getComparisonSize()).sum()
    val blockSizes = blocksAfterFiltering.map(_.size).sum()

    profilesStatsBrd.unpersist()
    profilesIds.unpersist()
    blockIndex.unpersist()
    profileBlocksNumIndex.unpersist()
    gt.unpersist()
    invProfileBlockSizeIndex.unpersist()
    comparisonsPerProfileIndex.unpersist()


    val d2size = {
      if (dataset.dtype == "clean") {
        dataset2.count()
      }
      else {
        0
      }
    }

    (dataset.name, dataset1.count(), d2size, newGT.size, numBlocks, blockSizes, numComp, pc, pq, numEdges, ((t2.getTimeInMillis - t1.getTimeInMillis) / 1000.0) / 60.0)
  }


  def main(args: Array[String]): Unit = {
    val config = Source.fromFile("/home/app/config/config.ini")
    val spark_max_memory = config.getLines().filter(_.startsWith("max_memory=")).next().replace("max_memory=", "")
    config.close()

    val input = Source.fromFile("/home/app/datasets/datasets.json")
    val lines = input.mkString
    input.close()
    val json = JSON.parseFull(lines)

    val datasets = json.get.asInstanceOf[List[Map[String, String]]].map(d => {
      val name = d("name")
      val basePath = "/home/app/" + d("base_path") + "/"
      val dataset1 = d("d1")
      val dataset2 = d.getOrElse("d2", "")
      val groundtruth = d("gt")
      val format = d("format")
      val dtype = d("type")
      val idField = d("d1_id_field")
      val gt_d1_field = d("gt_d1_field")
      val gt_d2_field = d("gt_d2_field")
      val purging_threshold = d.getOrElse("purging_threshold", "1.0").toDouble

      Dataset(name, basePath, dataset1, dataset2, groundtruth, format, dtype, gt_d1_field, gt_d2_field, idField = idField, purgingThreshold = purging_threshold)
    })

    val conf = new SparkConf()
      .setAppName("Main")
      .setMaster("local[*]")
      .set("spark.driver.memory", spark_max_memory)
      .set("spark.executor.memory", spark_max_memory)
      .set("spark.local.dir", "/home/app/tmp")
      .set("spark.driver.maxResultSize", "0")

    val outputPath = "/home/app/features/"

    val sc = new SparkContext(conf)

    val stats = datasets.map { d =>
      generateFeatures(d, outputPath)
    }
	
	Files.createDirectories(Paths.get("/home/app/results/"))

    val out = new PrintWriter("/home/app/results/01_blocking_performance.csv")
    val out2 = new PrintWriter("/home/app/results/01b_blocking_stats.csv")
    out.println("dataset,|e1|,|e2|,|D|,|C|,Recall,Precision,F1,RT (s)")
    out2.println("dataset,num_profiles,block_sizes_sum")

    stats.foreach(s => {
      out.println(List(s._1, s._2, s._3, s._4, s._7, s._8, s._9, 2 * ((s._8 * s._9) / (s._8 + s._9)), s._11).mkString(","))
      out2.println(List(s._1, s._2 + s._3, s._6).mkString(","))
    })

    out.close()
    out2.close()

    sc.stop()
  }
}
